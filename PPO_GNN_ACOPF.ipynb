{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Kx7uA5EO94c"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import time\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "base_dir = root_dir + ''\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(base_dir)\n",
        "print(os.getcwd())\n",
        "print(os.listdir())\n",
        "os.chdir(base_dir)\n",
        "print(os.getcwd())\n",
        "print(os.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ImD0CK9bBHiw",
        "outputId": "ded41423-cbea-4d52-f48d-31369f311ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypower in /usr/local/lib/python3.10/dist-packages (5.1.16)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypower\n",
        "import pypower\n",
        "from pypower.case14 import case14\n",
        "from pypower.case9 import case9\n",
        "from pypower.case118 import case118\n",
        "from pypower.case300 import case300\n",
        "\n",
        "from pypower.case9 import case9\n",
        "from pypower.case4gs import case4gs\n",
        "from pypower.case30 import case30\n",
        "\n",
        "from pypower.opf import opf\n",
        "from pypower.dcopf import dcopf\n",
        "from pypower.makeYbus import makeYbus\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy.optimize import minimize, fsolve\n",
        "import torch\n",
        "from scipy.optimize import Bounds\n",
        "import pandas as pd\n",
        "\n",
        "class environment():\n",
        "    def __init__(self, network):\n",
        "\n",
        "        #make bus numbers 0-N\n",
        "        df = pd.DataFrame([network['bus'][:,0]]).T\n",
        "        v = df.stack().unique()\n",
        "        v.sort()\n",
        "        f = pd.factorize(v)\n",
        "        m = pd.Series(f[0], f[1])\n",
        "        df = df.stack().map(m).unstack()\n",
        "        network['bus'][:,0] = df.to_numpy().reshape((len(df.to_numpy()),))\n",
        "        df = pd.DataFrame([network['gen'][:,0]]).T\n",
        "        df = df.stack().map(m).unstack()\n",
        "        network['gen'][:,0] = df.to_numpy().reshape((len(df.to_numpy()),))\n",
        "        df = pd.DataFrame([network['branch'][:,0]]).T\n",
        "        df = df.stack().map(m).unstack()\n",
        "        network['branch'][:,0] = df.to_numpy().reshape((len(df.to_numpy()),))\n",
        "        df = pd.DataFrame([network['branch'][:,1]]).T\n",
        "        df = df.stack().map(m).unstack()\n",
        "        network['branch'][:,1] = df.to_numpy().reshape((len(df.to_numpy()),))\n",
        "\n",
        "        if network['bus'][0][0]==1:\n",
        "            network['bus'][:,0] = network['bus'][:,0] - 1\n",
        "            network['gen'][:,0] = network['gen'][:,0] - 1\n",
        "            network['branch'][:,[0,1]] = network['branch'][:,[0,1]] - [1, 1]\n",
        "\n",
        "        self.network = network\n",
        "        self.num_buses = len(network['bus'])\n",
        "        self.num_gens = len(network['gen'])\n",
        "        self.num_branches = len(network['branch'])\n",
        "        self.t = 0\n",
        "        self.state = []\n",
        "        self.baseMVA = network['baseMVA']\n",
        "        self.balance = 0\n",
        "        self.limits = self.get_limits()\n",
        "        self.edge_index = torch.tensor(np.array([self.network['branch'][:, 0], self.network['branch'][:, 1]]), dtype=torch.int64)\n",
        "        self.ybus, _, _ = makeYbus(self.baseMVA, self.network['bus'], self.network['branch'])\n",
        "        self.max_load = np.sum(self.network['gen'][:,8]), np.sum(self.network['gen'][:,3])\n",
        "\n",
        "    #reset environment\n",
        "    def reset(self):\n",
        "        self.t = 0\n",
        "        self.update_state()\n",
        "        return self.state\n",
        "\n",
        "    #make a test set\n",
        "    def make_test_data(self):\n",
        "        X = []\n",
        "        Y = []\n",
        "        self.update_state()\n",
        "        for i in range(5001):\n",
        "            y = []\n",
        "            X.append(np.array(self.state).flatten())\n",
        "            print(np.shape(X))\n",
        "            self.change_load()\n",
        "            self.network = opf(self.network)\n",
        "            theta = self.network['bus'][:,8]\n",
        "            v = self.network['bus'][:,7]\n",
        "            P_G = self.network['gen'][:,1]/self.baseMVA\n",
        "            Q_G = self.network['gen'][:,2]/self.baseMVA\n",
        "            self.update_state()\n",
        "            y.extend(theta)\n",
        "            y.extend(v)\n",
        "            y.extend(P_G)\n",
        "            y.extend(Q_G)\n",
        "            Y.append(y)\n",
        "            if i % 5000 == 0:\n",
        "                np.savetxt('/content/gdrive/MyDrive/MTS Datasets/Regression/OPF/' + \"ACOPF_Case118Data_X_TEST\" + str(i) + \".csv\", np.array(X), delimiter =\",\")\n",
        "                np.savetxt('/content/gdrive/MyDrive/MTS Datasets/Regression/OPF/' + \"ACOPF_Case118Data_y_TEST\" + str(i) + \".csv\", Y, delimiter =\",\")\n",
        "\n",
        "\n",
        "    # get limits for inequality constraints\n",
        "    def get_limits(self):\n",
        "        lower = []\n",
        "        upper = []\n",
        "\n",
        "        for bus in self.network['bus']:\n",
        "            lower.append(-360)\n",
        "            upper.append(360)\n",
        "        for bus in self.network['bus']:\n",
        "            lower.append(bus[12])\n",
        "            upper.append(bus[11])\n",
        "        for gen in self.network['gen']:\n",
        "            lower.append(gen[9]/self.baseMVA)\n",
        "            upper.append(gen[8]/self.baseMVA)\n",
        "        for gen in self.network['gen']:\n",
        "            lower.append(gen[4]/self.baseMVA)\n",
        "            upper.append(gen[3]/self.baseMVA)\n",
        "        return lower, upper\n",
        "\n",
        "\n",
        "    #step from s_t to s_t+1,\n",
        "    def step(self, actions):\n",
        "        self.t += 1\n",
        "        self.change_load()\n",
        "        self.update_state()\n",
        "        self.ybus, _, _ = makeYbus(self.baseMVA, self.network['bus'], self.network['branch'])\n",
        "        return self.state, 0\n",
        "\n",
        "\n",
        "    def update_state(self):\n",
        "        x = []\n",
        "        for j in range(self.num_buses):\n",
        "            x.append([self.network['bus'][j][2], self.network['bus'][j][3]])\n",
        "        self.state = x\n",
        "\n",
        "\n",
        "    def change_load(self):\n",
        "        load = np.random.randint(self.max_load[0] * 0.1, self.max_load[0] * 0.85)\n",
        "        loads = (np.random.dirichlet(np.ones(self.num_buses)*4, size=1) * load)[0]\n",
        "        for i in range(self.num_buses):\n",
        "            if self.network['bus'][i][2] == 0:\n",
        "                continue\n",
        "            split = (random.randint(1, 2000) / 10000)\n",
        "            self.network['bus'][i][2] = loads[i] * (1-split)\n",
        "            self.network['bus'][i][3] = loads[i] * (split)\n",
        "\n",
        "    #in pypower.pipsopf_solver, add the following code after line 113:\n",
        "    #x0 = ppc['x0']\n",
        "    def run_opf(self, x0):\n",
        "        self.network['x0'] = x0\n",
        "        self.network = opf(self.network)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18lPJl_SrowJ"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric\n",
        "import os\n",
        "import time\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Categorical\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "class Actor(torch.nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Actor, self).__init__()\n",
        "        self.conv1 = GCNConv(state_dim[0], 64)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.conv3 = GCNConv(128, 64)\n",
        "        self.out = nn.Linear(state_dim[1] * 64, action_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.unsqueeze(0)\n",
        "        h = self.conv1(x, edge_index)\n",
        "        h = h.tanh()\n",
        "        h = self.conv2(h, edge_index)\n",
        "        h = h.tanh()\n",
        "        h = self.conv3(h, edge_index)\n",
        "        h = h.tanh()\n",
        "        out = self.out(torch.flatten(h, start_dim=1)).sigmoid()\n",
        "        return out\n",
        "\n",
        "# class Actor(torch.nn.Module):\n",
        "#     def __init__(self, state_dim, action_dim):\n",
        "#         super(Actor, self).__init__()\n",
        "#         self.linear1 = nn.Linear(state_dim[0] * state_dim[1], 64)\n",
        "#         self.linear2 = nn.Linear(64, 128)\n",
        "#         self.linear3 = nn.Linear(128, 64)\n",
        "#         self.out = nn.Linear(64, action_dim)\n",
        "\n",
        "#     def forward(self, x, edge_index):\n",
        "#         if len(x.shape) == 2:\n",
        "#             x = x.unsqueeze(0)\n",
        "#         h = self.linear1(x)\n",
        "#         h = h.tanh()\n",
        "#         h = self.linear2(h)\n",
        "#         h = h.tanh()\n",
        "#         h = self.linear3(h)\n",
        "#         h = h.tanh()\n",
        "#         out = self.out(h).sigmoid()\n",
        "#         return out\n",
        "\n",
        "\n",
        "class Critic(torch.nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        self.conv1 = GCNConv(state_dim[0], 64)\n",
        "        self.conv2 = GCNConv(64, 128)\n",
        "        self.conv3 = GCNConv(128, 64)\n",
        "        self.out = nn.Linear(state_dim[1] * 64, 1)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.unsqueeze(0)\n",
        "        h = self.conv1(x, edge_index)\n",
        "        h = h.tanh()\n",
        "        h = self.conv2(h, edge_index)\n",
        "        h = h.tanh()\n",
        "        h = self.conv3(h, edge_index)\n",
        "        h = h.tanh()\n",
        "        out = self.out(torch.flatten(h, start_dim=1))\n",
        "        return out\n",
        "\n",
        "# class Critic(torch.nn.Module):\n",
        "#     def __init__(self, state_dim):\n",
        "#         super(Critic, self).__init__()\n",
        "#         self.linear1 = nn.Linear(state_dim[0] * state_dim[1], 64)\n",
        "#         self.linear2 = nn.Linear(64, 128)\n",
        "#         self.linear3 = nn.Linear(128, 64)\n",
        "#         self.out = nn.Linear(64, 1)\n",
        "\n",
        "#     def forward(self, x, edge_index):\n",
        "#         if len(x.shape) == 2:\n",
        "#             x = x.unsqueeze(0)\n",
        "#         h = self.linear1(x)\n",
        "#         h = h.tanh()\n",
        "#         h = self.linear2(h)\n",
        "#         h = h.tanh()\n",
        "#         h = self.linear3(h)\n",
        "#         h = h.tanh()\n",
        "#         out = self.out(h)\n",
        "#         return out\n",
        "\n",
        "\n",
        "class Buffer:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.state_values = []\n",
        "        self.is_terminals = []\n",
        "\n",
        "\n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.state_values[:]\n",
        "        del self.is_terminals[:]\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, edge_index, action_sd_init):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.edge_index = edge_index.to(device)\n",
        "        self.action_dim = action_dim\n",
        "        self.action_var = torch.full((action_dim,), action_sd_init * action_sd_init).to(device)\n",
        "\n",
        "        self.actor = Actor(state_dim, action_dim)\n",
        "\n",
        "        self.critic = Critic(state_dim)\n",
        "\n",
        "    def set_action_sd(self, new_action_sd):\n",
        "        self.action_var = torch.full((self.action_dim,), new_action_sd * new_action_sd).to(device)\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        action_mean = self.actor(state, self.edge_index)\n",
        "        cov_mat = torch.diag(self.action_var).unsqueeze(dim=0).float()\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        state_val = self.critic(state, self.edge_index)\n",
        "\n",
        "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
        "\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        action_mean = self.actor(state, self.edge_index)\n",
        "        action_var = self.action_var.expand_as(action_mean)\n",
        "        cov_mat = torch.diag_embed(action_var).to(device)\n",
        "        dist = MultivariateNormal(action_mean, cov_mat)\n",
        "\n",
        "        if self.action_dim == 1:\n",
        "            action = action.reshape(-1, self.action_dim)\n",
        "\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        S = dist.entropy()\n",
        "        state_values = self.critic(state, self.edge_index)\n",
        "\n",
        "        return action_logprobs, state_values, S\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, epochs, eps_clip, edge_index, action_sd):\n",
        "\n",
        "        self.action_sd = action_sd\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.buffer = Buffer()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, edge_index, action_sd).to(device)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ])\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, edge_index, action_sd).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "\n",
        "    def set_action_sd(self, new_action_sd):\n",
        "\n",
        "        self.action_sd = new_action_sd\n",
        "        self.policy.set_action_sd(new_action_sd)\n",
        "        self.policy_old.set_action_sd(new_action_sd)\n",
        "\n",
        "\n",
        "    def decay_action_sd(self, action_sd_decay_rate, min_action_sd):\n",
        "        self.action_sd = self.action_sd - action_sd_decay_rate\n",
        "        self.action_sd = round(self.action_sd, 4)\n",
        "        if (self.action_sd <= min_action_sd):\n",
        "            self.action_sd = min_action_sd\n",
        "            print(\"action_sd : \", self.action_sd)\n",
        "        else:\n",
        "            print(\"action_sd : \", self.action_sd)\n",
        "        self.set_action_sd(self.action_sd)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        with torch.no_grad():\n",
        "            state = torch.FloatTensor(state).to(device)\n",
        "            action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "        self.buffer.states.append(state)\n",
        "        self.buffer.actions.append(action)\n",
        "        self.buffer.logprobs.append(action_logprob)\n",
        "        self.buffer.state_values.append(state_val)\n",
        "\n",
        "        return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        # estimate of returns\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        #  to tensor\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
        "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
        "\n",
        "        # calculate advantages\n",
        "        advantages = rewards.detach() - old_state_values.detach()\n",
        "\n",
        "\n",
        "        # Optimize policy\n",
        "        for _ in range(self.epochs):\n",
        "\n",
        "            # Evaluate old actions and values\n",
        "            logprobs, state_values, S = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "\n",
        "            # ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            surrogate1 = ratios * advantages\n",
        "            surrogate2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            # objective function\n",
        "            loss = -torch.min(surrogate1, surrogate2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * S\n",
        "\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.buffer.clear()\n",
        "\n",
        "\n",
        "    def save(self, checkpoint_path):\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "\n",
        "\n",
        "    def load(self, checkpoint_path):\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px261tRDS7iI"
      },
      "outputs": [],
      "source": [
        "# initialize hyperparameters\n",
        "\n",
        "env_name = \"Case 118\"\n",
        "\n",
        "episode_len = 1000\n",
        "max_training_timesteps = episode_len*100\n",
        "\n",
        "print_freq = episode_len\n",
        "log_freq = episode_len\n",
        "save_model_freq = 250\n",
        "\n",
        "action_sd = 0.25     #action standard deviation\n",
        "action_sd_decay_rate = 0.01\n",
        "min_action_sd = 0.0000000001\n",
        "action_sd_decay_freq = 250\n",
        "\n",
        "T = episode_len * 1     # update policy every n timesteps\n",
        "epochs = 100               # update policy for K epochs\n",
        "eps_clip = 0.1             # clip parameter for PPO\n",
        "gamma = 0.001              # discount factor\n",
        "lr_actor = 0.000005      # learning rate for actor network\n",
        "lr_critic = 0.000005      # learning rate for critic network\n",
        "\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "\n",
        "\n",
        "env = environment(case118())\n",
        "# env = environment(case14())\n",
        "\n",
        "limits = env.get_limits()\n",
        "\n",
        "# state space dimension\n",
        "state_dim = [2, env.num_buses]\n",
        "# action space dimension\n",
        "action_dim = 2 * env.num_buses + 2 * env.num_gens\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\"\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "directory = directory + '/' + env_name + '/'\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "\n",
        "checkpoint_path = directory + \"PPO_{}.pth\".format(env_name)\n",
        "\n",
        "if random_seed:\n",
        "    torch.manual_seed(random_seed)\n",
        "    env.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "\n",
        "# initialize agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, epochs, eps_clip, env.edge_index, action_sd)\n",
        "# ppo_agent.load()\n",
        "\n",
        "\n",
        "\n",
        "print_running_reward = 0\n",
        "print_running_episodes = 0\n",
        "\n",
        "\n",
        "t = 0\n",
        "i_episode = 0\n",
        "\n",
        "log_data = []\n",
        "\n",
        "# training loop\n",
        "\n",
        "while t <= max_training_timesteps:\n",
        "\n",
        "    state = env.reset()\n",
        "    current_ep_reward = 0\n",
        "    diverge_counter = 0\n",
        "    converge_times = 0\n",
        "\n",
        "    for t in range(0, episode_len):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        for i in range(len(action)):\n",
        "            ll = limits[0][i]\n",
        "            uu = limits[1][i]\n",
        "            action[i] = action[i] * (uu - ll) + ll\n",
        "\n",
        "        reward = time.time()\n",
        "        env.run_opf(action)\n",
        "\n",
        "        # rewards\n",
        "        reward = -(time.time() - reward)\n",
        "        if env.network['success'] == False:\n",
        "            diverge_counter = diverge_counter + 1\n",
        "            reward = -5\n",
        "        else:\n",
        "            converge_times = converge_times - reward\n",
        "\n",
        "        state, done = env.step(action)\n",
        "        # saving reward\n",
        "        ppo_agent.buffer.rewards.append(reward)\n",
        "        ppo_agent.buffer.is_terminals.append(done)\n",
        "\n",
        "        t +=1\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        # update agent\n",
        "        if t % T == 0:\n",
        "            ppo_agent.update()\n",
        "\n",
        "        # decay action sd of ouput action distribution\n",
        "        if t % action_sd_decay_freq == 0:\n",
        "            ppo_agent.decay_action_sd(action_sd_decay_rate, min_action_sd)\n",
        "\n",
        "\n",
        "        # printing average reward\n",
        "        if t % print_freq == 0:\n",
        "            print(converge_times/(print_freq-diverge_counter), diverge_counter)\n",
        "            log_data.append([diverge_counter, converge_times/(print_freq-diverge_counter), current_ep_reward / print_freq])\n",
        "\n",
        "            print_avg_reward = current_ep_reward / print_freq\n",
        "            print_avg_reward = round(print_avg_reward, 2)\n",
        "\n",
        "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, t, print_avg_reward))\n",
        "\n",
        "            np.savetxt(checkpoint_path + \"log_data\", log_data, delimiter =\", \", fmt ='% s')\n",
        "\n",
        "            print_running_reward = 0\n",
        "            print_running_episodes = 0\n",
        "            diverge_counter = 0\n",
        "            converge_times = 0\n",
        "\n",
        "        # save model\n",
        "        if t % save_model_freq == 0:\n",
        "            print(\"saving model at : \" + checkpoint_path)\n",
        "            ppo_agent.save(checkpoint_path)\n",
        "\n",
        "    print_running_reward += current_ep_reward\n",
        "    print_running_episodes += 1\n",
        "\n",
        "\n",
        "    i_episode += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKydo1xmJY7G"
      },
      "outputs": [],
      "source": [
        "env = environment(case118())\n",
        "X = np.loadtxt('ACOPF_Case14Data_X_TEST5000.csv', delimiter=\",\", dtype=None)\n",
        "divergences = 0\n",
        "r_total = 0\n",
        "div_count = 0\n",
        "for i in range(len(X)):\n",
        "    x = X[i]\n",
        "    state = []\n",
        "    for j in range(int(len(x)/2)):\n",
        "        state.append([x[j*2], x[j*2+1]])\n",
        "        env.network['bus'][j][2] = x[j*2]\n",
        "        env.network['bus'][j][3] = x[j*2+1]\n",
        "    action = ppo_agent.select_action(state)\n",
        "    for k in range(len(action)):\n",
        "        ll = limits[0][k]\n",
        "        uu = limits[1][k]\n",
        "        action[k] = action[k] * (uu - ll) + ll\n",
        "    r = time.time()\n",
        "    env.run_opf(action)\n",
        "    r = time.time() - r\n",
        "    if env.network['success'] == False:\n",
        "        div_count+=1\n",
        "    r_total += r"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}